{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Module 1: Imports & Configuration\n",
        "---------------------------------\n",
        "- Import libraries\n",
        "- Define global parameters\n",
        "- Added learning rate scheduler for adaptive training\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Gr4z9ZlVFd_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "torch.set_default_dtype(torch.float32)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Global config for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "QxAClJvNFjqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch between methods: 'finite_agent', 'discrete_state', 'projection'\n",
        "method = 'finite_agent'\n",
        "\n",
        "# Simulation parameters\n",
        "N_agents = 50            # for finite-agent method\n",
        "N_grid = 100             # for discrete-state grid\n",
        "N_basis = 4              # for projection method\n",
        "epochs = 2000            # training epochs\n",
        "batch_size = 2048        # PDE collocation samples\n",
        "lr = 1e-3                # initial learning rate\n",
        "lr_min = 1e-5            # minimum learning rate for scheduler\n",
        "verbose = True\n",
        "\n",
        "# Save output figures to this folder\n",
        "output_dir = \"./eminn_results/\"\n",
        "import os\n",
        "os.makedirs(output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "85OX_u3BFq_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Module 2: Economic Environment (Continuous-Time Krusellâ€“Smith)\n",
        "-------------------------------------------------------------\n",
        "- Defines agent dynamics, utility functions, and aggregate shock process\n",
        "- Includes soft borrowing constraint penalties\n",
        "- Added idiosyncratic wealth diffusion (sigma_w)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ouMpTiXaFsqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EconomicEnvironment:\n",
        "    def __init__(self, params):\n",
        "        # Preferences & returns\n",
        "        self.beta = params.get('beta', 0.98)\n",
        "        self.r = params.get('r', 0.03)           # interest rate\n",
        "        self.wage = params.get('wage', 1.0)      # wage per efficiency unit\n",
        "\n",
        "        # Aggregate shock (log productivity z)\n",
        "        self.rho = params.get('rho', 0.95)\n",
        "        self.sigma_z = params.get('sigma_z', 0.02)\n",
        "        self.z0 = params.get('z0', 0.0)          # mean of log z\n",
        "\n",
        "        # Idiosyncratic wealth domain\n",
        "        self.w_min = params.get('w_min', 0.0)\n",
        "        self.w_max = params.get('w_max', 5.0)\n",
        "        self.sigma_w = params.get('sigma_w', 0.01)  # idiosyncratic wealth diffusion\n",
        "\n",
        "        # Preferences\n",
        "        self.sigma = params.get('sigma', 2.0)    # CRRA\n",
        "        self.lambda_pen = params.get('lambda_pen', 10.0)  # boundary penalty\n",
        "\n",
        "    # -------------------------------\n",
        "    # Utility and policy\n",
        "    # -------------------------------\n",
        "    def utility(self, c):\n",
        "        \"\"\" CRRA utility function \"\"\"\n",
        "        return (c.pow(1 - self.sigma) - 1) / (1 - self.sigma)\n",
        "\n",
        "    def solve_consumption(self, V_w):\n",
        "        \"\"\" Optimal consumption: u'(c) = V_w => c = V_w^{-1/sigma} \"\"\"\n",
        "        return V_w.clamp(min=1e-8).pow(-1.0/self.sigma)\n",
        "\n",
        "    def apply_boundary_penalty(self, w):\n",
        "        \"\"\" Quadratic soft penalty for borrowing constraint violation \"\"\"\n",
        "        penalty = torch.where(\n",
        "            w < self.w_min,\n",
        "            -self.lambda_pen * (self.w_min - w).pow(2),\n",
        "            torch.zeros_like(w)\n",
        "        )\n",
        "        return penalty\n",
        "\n",
        "    # -------------------------------\n",
        "    # Drift functions for HJB/KFE\n",
        "    # -------------------------------\n",
        "    def wealth_drift(self, w, c, z):\n",
        "        \"\"\"\n",
        "        Drift of wealth under continuous-time KS:\n",
        "        dw/dt = r*w + wage*exp(z) - c\n",
        "        \"\"\"\n",
        "        return self.r * w + self.wage * torch.exp(z) - c\n",
        "\n",
        "    def aggregate_drift(self, z):\n",
        "        \"\"\" AR(1) in continuous time: dz = rho*(z0 - z) dt + sigma dB \"\"\"\n",
        "        return self.rho * (self.z0 - z)"
      ],
      "metadata": {
        "id": "OmUHttsCHDKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Module 3: Distribution Approximations\n",
        "-------------------------------------\n",
        "Implements three methods:\n",
        "1. Finite-Agent simulation\n",
        "2. Discrete-State histogram\n",
        "3. Projection on moments\n",
        "These provide distribution features (phi) for the EMINN master PDE.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CsW1xImxHFAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# ====================================================\n",
        "# 3A. Finite-Agent Approximation\n",
        "# ====================================================\n",
        "class FiniteAgentDistribution:\n",
        "    def __init__(self, env, N_agents=50):\n",
        "        self.env = env\n",
        "        self.N = N_agents\n",
        "        self.w = torch.rand(self.N) * (env.w_max - env.w_min) + env.w_min\n",
        "        self.z = torch.tensor(env.z0, dtype=torch.float32)  # scalar tensor\n",
        "\n",
        "    def evolve(self, policy_fn, dt=0.01):\n",
        "        phi = self.extract_features()\n",
        "        w_tensor = self.w.clone()\n",
        "        c = policy_fn(w_tensor, self.z.repeat(self.N), phi.repeat(self.N, 1))\n",
        "\n",
        "        drift_w = self.env.wealth_drift(self.w, c, self.z)\n",
        "        noise_w = self.env.sigma_w * np.sqrt(dt) * torch.randn(self.N)\n",
        "        self.w += drift_w * dt + noise_w\n",
        "        self.w = self.w.clamp(min=self.env.w_min)\n",
        "\n",
        "        # Aggregate shock update\n",
        "        noise_z = self.env.sigma_z * np.sqrt(dt) * torch.randn(1).item()\n",
        "        self.z += self.env.aggregate_drift(self.z) * dt + noise_z\n",
        "\n",
        "    def extract_features(self):\n",
        "        mean_w = torch.mean(self.w)\n",
        "        var_w = torch.var(self.w)\n",
        "        skew_w = torch.mean((self.w - mean_w) ** 3) / (torch.std(self.w) ** 3 + 1e-8)\n",
        "        return torch.tensor([mean_w, var_w, skew_w]).unsqueeze(0)"
      ],
      "metadata": {
        "id": "OBHsYBp3HHhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 3B. Discrete-State Histogram Approximation\n",
        "# ====================================================\n",
        "class DiscreteStateDistribution:\n",
        "    def __init__(self, env, N_grid=100):\n",
        "        self.env = env\n",
        "        self.Ng = N_grid\n",
        "        self.grid = torch.linspace(env.w_min, env.w_max, N_grid)\n",
        "        self.mass = torch.ones(N_grid) / N_grid\n",
        "        self.z = torch.tensor(env.z0, dtype=torch.float32)\n",
        "\n",
        "    def evolve(self, policy_fn, dt=0.01):\n",
        "        drift = torch.zeros_like(self.mass)\n",
        "        phi = self.extract_features()\n",
        "        for i, w in enumerate(self.grid):\n",
        "            c = policy_fn(w.unsqueeze(0), self.z.unsqueeze(0), phi)\n",
        "            mu_w = self.env.wealth_drift(w, c, self.z)\n",
        "            idx_next = min(self.Ng - 1, max(0, int(i + mu_w.item() * dt / self.grid[1].item())))\n",
        "            drift[idx_next] += self.mass[i]\n",
        "\n",
        "        self.mass = drift / torch.sum(drift)\n",
        "\n",
        "        # Aggregate shock update\n",
        "        noise = self.env.sigma_z * np.sqrt(dt) * torch.randn(1).item()\n",
        "        self.z += self.env.aggregate_drift(self.z) * dt + noise\n",
        "\n",
        "    def extract_features(self):\n",
        "        mean_w = torch.sum(self.mass * self.grid)\n",
        "        var_w = torch.sum(self.mass * (self.grid - mean_w) ** 2)\n",
        "        skew_w = torch.sum(self.mass * (self.grid - mean_w) ** 3) / (var_w.sqrt() ** 3 + 1e-8)\n",
        "        return torch.tensor([mean_w, var_w, skew_w]).unsqueeze(0)"
      ],
      "metadata": {
        "id": "0NYUQ6ctHJuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================================\n",
        "# 3C. Projection-on-Moments Approximation\n",
        "# ====================================================\n",
        "class ProjectionDistribution:\n",
        "    def __init__(self, env, N_basis=3):\n",
        "        self.env = env\n",
        "        self.Nb = N_basis\n",
        "        self.coeffs = torch.zeros(N_basis)\n",
        "        self.z = torch.tensor(env.z0, dtype=torch.float32)\n",
        "\n",
        "    def evolve(self, policy_fn, dt=0.01):\n",
        "        w_samples = torch.rand(500) * (self.env.w_max - self.env.w_min) + self.env.w_min\n",
        "        phi = self.extract_features()\n",
        "        c = policy_fn(w_samples, self.z.repeat(500), phi.repeat(500, 1))\n",
        "        w_next = w_samples + self.env.wealth_drift(w_samples, c, self.z) * dt + \\\n",
        "                 self.env.sigma_w * np.sqrt(dt) * torch.randn(500)\n",
        "\n",
        "        mean_w = torch.mean(w_next)\n",
        "        var_w = torch.var(w_next)\n",
        "        skew_w = torch.mean((w_next - mean_w) ** 3) / (torch.std(w_next) ** 3 + 1e-8)\n",
        "        self.coeffs = torch.tensor([mean_w, var_w, skew_w])\n",
        "\n",
        "        # Aggregate shock update\n",
        "        noise = self.env.sigma_z * np.sqrt(dt) * torch.randn(1).item()\n",
        "        self.z += self.env.aggregate_drift(self.z) * dt + noise\n",
        "\n",
        "    def extract_features(self):\n",
        "        return self.coeffs.unsqueeze(0)"
      ],
      "metadata": {
        "id": "myyBF6VKHLnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Module 4: Neural Network & PDE Residual\n",
        "---------------------------------------\n",
        "- ValueNet: Approximates value function V(w,z,phi)\n",
        "- Computes HJB + KFE PDE residual for EMINN training\n",
        "- Adds shape penalties: monotonicity (V_w >= 0) and concavity (V_ww <= 0)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sicfJl17HMt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# ==========================================\n",
        "# 4A. Value Function Network\n",
        "# ==========================================\n",
        "class ValueNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_layers=[128,128,128,128]):\n",
        "        super(ValueNet, self).__init__()\n",
        "        layers = []\n",
        "        dim = input_dim\n",
        "        for h in hidden_layers:\n",
        "            layers.append(nn.Linear(dim, h))\n",
        "            layers.append(nn.Tanh())  # smooth activation for PDEs\n",
        "            dim = h\n",
        "        layers.append(nn.Linear(dim, 1))\n",
        "        layers.append(nn.Softplus())  # ensure nonnegative output\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)"
      ],
      "metadata": {
        "id": "njdVkGAKHOgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4B. Policy Function from Value Network\n",
        "# ==========================================\n",
        "def policy_function(V_net, env, w, z, phi):\n",
        "    \"\"\"\n",
        "    Compute optimal consumption policy via FOC:\n",
        "    u'(c) = V_w => c = V_w^{-1/sigma}\n",
        "    \"\"\"\n",
        "    inputs = torch.cat([w.unsqueeze(-1), z.unsqueeze(-1), phi], dim=1)\n",
        "    V = V_net(inputs)\n",
        "    V_w = torch.autograd.grad(V.sum(), w, create_graph=True)[0]\n",
        "    c = env.solve_consumption(V_w)\n",
        "    return c"
      ],
      "metadata": {
        "id": "PaqNb8XrHP_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4C. Master PDE Residual\n",
        "# ==========================================\n",
        "def compute_pde_residual(V_net, env, states, phi_drift_fn, method='finite_agent'):\n",
        "    \"\"\"\n",
        "    Compute PDE residual for HJB equation and derivatives V_w, V_ww with numerical safeguards\n",
        "    \"\"\"\n",
        "    w = states[:, 0].requires_grad_(True)\n",
        "    z = states[:, 1].requires_grad_(True)  # Enable grad for z\n",
        "    phi = states[:, 2:].requires_grad_(True)  # Enable grad for phi\n",
        "\n",
        "    inputs = torch.cat([w.unsqueeze(-1), z.unsqueeze(-1), phi], dim=1)\n",
        "    V = V_net(inputs)\n",
        "    V_w = torch.autograd.grad(V.sum(), w, create_graph=True)[0]\n",
        "    V_ww = torch.autograd.grad(V_w.sum(), w, create_graph=True)[0]\n",
        "    V_z = torch.autograd.grad(V.sum(), z, create_graph=True)[0]\n",
        "    V_zz = torch.autograd.grad(V_z.sum(), z, create_graph=True)[0]\n",
        "\n",
        "    # Clamp gradients to prevent explosion\n",
        "    V_w = torch.clamp(V_w, -1e6, 1e6)\n",
        "    V_ww = torch.clamp(V_ww, -1e6, 1e6)\n",
        "    V_z = torch.clamp(V_z, -1e6, 1e6)\n",
        "    V_zz = torch.clamp(V_zz, -1e6, 1e6)\n",
        "\n",
        "    c = env.solve_consumption(V_w)\n",
        "    c = torch.clamp(c, 1e-6, 1e6)  # Prevent zero or negative consumption\n",
        "    mu_w = env.wealth_drift(w, c, z)\n",
        "    mu_z = -env.rho * z\n",
        "    sigma_z = env.sigma_z\n",
        "    sigma_w = env.sigma_w\n",
        "\n",
        "    if method == 'projection':\n",
        "        V_phi = torch.autograd.grad(V.sum(), phi, create_graph=True)[0]\n",
        "        V_phi = torch.clamp(V_phi, -1e6, 1e6)  # Clamp V_phi\n",
        "        phi_drift = phi_drift_fn(w, z, phi, V_w, c, method=method)\n",
        "        phi_drift = torch.clamp(phi_drift, -1e6, 1e6)  # Clamp phi_drift\n",
        "        pde_residual = env.r * V - (\n",
        "            env.utility(c) +\n",
        "            mu_w * V_w +\n",
        "            0.5 * (sigma_w**2) * V_ww +\n",
        "            mu_z * V_z +\n",
        "            0.5 * (sigma_z**2) * V_zz +\n",
        "            torch.sum(phi_drift * V_phi, dim=1, keepdim=True)\n",
        "        )\n",
        "    else:\n",
        "        pde_residual = env.r * V - (\n",
        "            env.utility(c) +\n",
        "            mu_w * V_w +\n",
        "            0.5 * (sigma_w**2) * V_ww +\n",
        "            mu_z * V_z +\n",
        "            0.5 * (sigma_z**2) * V_zz\n",
        "        )\n",
        "\n",
        "    # Replace nan with zero in residual\n",
        "    pde_residual = torch.where(torch.isnan(pde_residual) | torch.isinf(pde_residual), torch.zeros_like(pde_residual), pde_residual)\n",
        "\n",
        "    # Debug: Check requires_grad\n",
        "    if not pde_residual.requires_grad:\n",
        "        print(\"Warning: pde_residual does not require grad\")\n",
        "    if not V_w.requires_grad:\n",
        "        print(\"Warning: V_w does not require grad\")\n",
        "    if not V_ww.requires_grad:\n",
        "        print(\"Warning: V_ww does not require grad\")\n",
        "\n",
        "    return pde_residual, V_w, V_ww"
      ],
      "metadata": {
        "id": "vWR7J9oZHU9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 4D. Shape Penalty Function\n",
        "# ==========================================\n",
        "def shape_penalties(V_w, V_ww, kappa_concavity=0.01, kappa_monotonic=0.01):\n",
        "    \"\"\"\n",
        "    Compute shape penalties:\n",
        "    - concavity: max(V_ww, 0)\n",
        "    - monotonicity: max(-V_w, 0)\n",
        "    \"\"\"\n",
        "    loss_concavity = kappa_concavity * torch.mean(F.relu(V_ww))\n",
        "    loss_monotonic = kappa_monotonic * torch.mean(F.relu(-V_w))\n",
        "    return loss_concavity, loss_monotonic"
      ],
      "metadata": {
        "id": "EClvlAZcHeO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# ==================================================\n",
        "# 5A. Sampling Function\n",
        "# =================================================="
      ],
      "metadata": {
        "id": "vdARw2R4HZrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Module 5: Sampling & Training Loop\n",
        "----------------------------------\n",
        "- Implements Algorithm 1 from the EMINN paper\n",
        "- Integrates full PDE residual, shape penalties, and adaptive sampling\n",
        "- Added explicit moment, steady-state, and ergodic sampling\n",
        "\"\"\"\n",
        "def sample_states(env, batch_size, method='finite_agent'):\n",
        "    \"\"\"\n",
        "    Sample states for PDE residual computation with gradient support\n",
        "    \"\"\"\n",
        "    w = torch.linspace(env.w_min, env.w_max, batch_size).to(device).requires_grad_(True)\n",
        "    z = torch.linspace(env.z0 - 0.05, env.z0 + 0.05, batch_size).to(device).requires_grad_(True)\n",
        "    mean_init = env.wage * np.exp(env.z0) / env.r  # Steady-state mean\n",
        "    var_init = (env.sigma_w ** 2) / (2 * env.r)    # Steady-state variance\n",
        "    if method == 'projection':\n",
        "        phi = torch.tensor([[mean_init, var_init, 0.0]] * batch_size, dtype=torch.float32).to(device).requires_grad_(True)\n",
        "    else:\n",
        "        phi = torch.zeros(batch_size, 3).to(device).requires_grad_(True)\n",
        "    return torch.cat([w.unsqueeze(-1), z.unsqueeze(-1), phi], dim=1)"
      ],
      "metadata": {
        "id": "UiFJ7Wq3Hied"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# 5B. Phi Drift Function for KFE Term\n",
        "# ==================================================\n",
        "def phi_drift_fn(w, z, phi, V_w, c, method='finite_agent'):\n",
        "    \"\"\"\n",
        "    Compute dphi/dt (L_phi) for each method based on KFE.\n",
        "    Uses wealth drift and consumption policy to compute moment dynamics.\n",
        "    Returns shape [batch_size, 3] to match phi.\n",
        "    \"\"\"\n",
        "    if phi.shape[1] == 0:\n",
        "        return torch.zeros_like(phi)\n",
        "\n",
        "    # Compute wealth drift\n",
        "    mu_w = env.wealth_drift(w, c, z)  # shape: [batch_size]\n",
        "    sigma_w = env.sigma_w\n",
        "\n",
        "    # Moment drifts based on KFE\n",
        "    if method in ['finite_agent', 'discrete_state', 'projection']:\n",
        "        # Mean: E[mu_w]\n",
        "        phi_dot_mean = torch.mean(mu_w)  # scalar\n",
        "        # Variance: E[(w - E[w])^2 mu_w] + E[sigma_w^2]\n",
        "        mean_w = phi[:, 0]  # shape: [batch_size]\n",
        "        var_dot = torch.mean((w - mean_w) * mu_w) + sigma_w**2  # scalar\n",
        "        # Skewness: approximate via moment dynamics\n",
        "        skew_dot = torch.mean((w - mean_w)**3 * mu_w) / (phi[:, 1].sqrt()**3 + 1e-8)  # scalar\n",
        "        # Broadcast scalars to match phi's shape [batch_size, 3]\n",
        "        phi_dot = torch.stack([\n",
        "            phi_dot_mean.expand(phi.shape[0]),\n",
        "            var_dot.expand(phi.shape[0]),\n",
        "            skew_dot.expand(phi.shape[0])\n",
        "        ], dim=1)  # shape: [batch_size, 3]\n",
        "        return phi_dot\n",
        "    else:\n",
        "        return torch.zeros_like(phi)"
      ],
      "metadata": {
        "id": "gRZRHFqVHksZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================================\n",
        "# 5C. Training Loop (Algorithm 1)\n",
        "# ==================================================\n",
        "def train_eminn(V_net, env, epochs=1000, batch_size=2048, lr=1e-3, method='finite_agent', verbose=False):\n",
        "    \"\"\"\n",
        "    Train ValueNet on master PDE using Adam optimizer with gradient clipping and debugging.\n",
        "    \"\"\"\n",
        "    optimizer = torch.optim.Adam(V_net.parameters(), lr=lr)\n",
        "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[200, 300, 400], gamma=0.5)\n",
        "    history = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        states = sample_states(env, batch_size, method=method)\n",
        "        # Ensure states requires grad\n",
        "        states = states.requires_grad_(True)\n",
        "        residual, V_w, V_ww = compute_pde_residual(V_net, env, states, phi_drift_fn=phi_drift_fn, method=method)\n",
        "        pde_loss = torch.mean(residual**2)\n",
        "        loss = pde_loss + env.lambda_pen * torch.mean(V_w**2)\n",
        "\n",
        "        # Check for nan or inf\n",
        "        if torch.isnan(loss) or torch.isinf(loss):\n",
        "            print(f\"Epoch {epoch+1}: Warning: Loss is nan or inf, skipping update\")\n",
        "            continue\n",
        "\n",
        "        # Debug: Check requires_grad\n",
        "        if not loss.requires_grad:\n",
        "            print(f\"Epoch {epoch+1}: Warning: Loss does not require grad\")\n",
        "            print(f\"states.requires_grad: {states.requires_grad}\")\n",
        "            print(f\"residual.requires_grad: {residual.requires_grad}\")\n",
        "            print(f\"V_w.requires_grad: {V_w.requires_grad}\")\n",
        "\n",
        "        try:\n",
        "            loss.backward()\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Epoch {epoch+1}: Backward error: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Apply gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(V_net.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        history.append(loss.item())\n",
        "        if verbose and (epoch + 1) % 100 == 0:\n",
        "            print(f\"Epoch {epoch+1:4d}: Loss={loss.item():.6f}  PDE={pde_loss.item():.6f}  LR={scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "    return V_net, history"
      ],
      "metadata": {
        "id": "bikROfTDHmkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Module 6: Policy Extraction & Simulation\n",
        "----------------------------------------\n",
        "- Extracts optimal policy (consumption) from trained ValueNet\n",
        "- Simulates wealth distribution dynamics over time\n",
        "- Collects paths of wealth, distribution moments, and aggregate variables\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "8aH-h08GHoYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_policy_function(V_net, env):\n",
        "    \"\"\"\n",
        "    Extract policy function (consumption) from ValueNet, accepting a single inputs tensor\n",
        "    \"\"\"\n",
        "    def policy_fn(inputs):\n",
        "        # inputs: [batch_size, 5] = [w, z, phi[0], phi[1], phi[2]]\n",
        "        inputs = inputs.clone().requires_grad_(True)  # Ensure grad tracking\n",
        "        V = V_net(inputs)\n",
        "        V_w = torch.autograd.grad(V.sum(), inputs, create_graph=False)[0][:, 0]  # Gradient w.r.t. w (first column)\n",
        "        c = env.solve_consumption(V_w)\n",
        "        c = torch.clamp(c, 1e-6, 1e6)  # Prevent invalid consumption\n",
        "        # Debug: Check requires_grad\n",
        "        if V_w is None:\n",
        "            print(\"Error: V_w is None in policy_fn\")\n",
        "        return c\n",
        "    return policy_fn"
      ],
      "metadata": {
        "id": "HqlnKmCOHqJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 6B. Economy Simulation\n",
        "# ==========================================\n",
        "def simulate_economy(env, policy_fn, method='finite_agent', T=100, dt=0.01, N_agents=50, N_grid=100, N_basis=3):\n",
        "    \"\"\"\n",
        "    Simulate wealth and aggregate shock trajectories using the learned policy.\n",
        "    Returns wealth paths, z paths, and distribution moments with numerical safeguards.\n",
        "    \"\"\"\n",
        "    # Initialize paths\n",
        "    T_steps = int(T / dt)\n",
        "    wealth_paths = []\n",
        "    z_path = [env.z0]\n",
        "    phi_path = []\n",
        "    z = torch.tensor(env.z0, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Analytical steady-state mean for initialization\n",
        "    mean_init = env.wage * np.exp(env.z0) / env.r  # e.g., (1.0 * 1.0) / 0.03 â‰ˆ 33.33\n",
        "    var_init = (env.sigma_w ** 2) / (2 * env.r)    # e.g., (0.01 ** 2) / (2 * 0.03) â‰ˆ 0.001667\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if method == 'finite_agent':\n",
        "            # Initialize N_agents wealths\n",
        "            wealth = torch.linspace(env.w_min, env.w_max, N_agents).to(device)\n",
        "            wealth_paths.append(wealth.cpu().numpy())\n",
        "            phi_init = torch.tensor([torch.mean(wealth), torch.var(wealth), 0.0], dtype=torch.float32).to(device)\n",
        "            phi_path.append(phi_init.unsqueeze(0))\n",
        "        elif method == 'discrete_state':\n",
        "            # Initialize histogram-based distribution\n",
        "            wealth = torch.linspace(env.w_min, env.w_max, N_grid).to(device)\n",
        "            weights = torch.ones(N_grid) / N_grid\n",
        "            phi_init = torch.tensor([torch.mean(wealth), torch.var(wealth), 0.0], dtype=torch.float32).to(device)\n",
        "            wealth_paths.append(phi_init.cpu().numpy())\n",
        "            phi_path.append(phi_init.unsqueeze(0))\n",
        "        elif method == 'projection':\n",
        "            # Initialize moment-based distribution with steady-state values\n",
        "            phi_init = torch.tensor([mean_init, var_init, 0.0], dtype=torch.float32).to(device)\n",
        "            wealth_paths.append(phi_init.cpu().numpy())\n",
        "            phi_path.append(phi_init.unsqueeze(0))\n",
        "        else:\n",
        "            raise ValueError(\"Unknown method\")\n",
        "\n",
        "    # Simulate dynamics\n",
        "    for t in range(T_steps - 1):\n",
        "        if method == 'finite_agent':\n",
        "            inputs = torch.cat([\n",
        "                wealth.unsqueeze(-1),\n",
        "                z.repeat(N_agents).unsqueeze(-1),\n",
        "                phi_path[-1].repeat(N_agents, 1)\n",
        "            ], dim=1).requires_grad_(True)  # Enable gradients for inputs\n",
        "            c = policy_fn(inputs)\n",
        "            with torch.no_grad():\n",
        "                dw = env.wealth_drift(wealth, c, z) * dt + env.sigma_w * torch.sqrt(torch.tensor(dt, dtype=torch.float32)) * torch.randn(N_agents).to(device)\n",
        "                wealth = wealth + dw\n",
        "                wealth = torch.clamp(wealth, env.w_min, env.w_max)\n",
        "                wealth_paths.append(wealth.cpu().numpy())\n",
        "                phi_new = torch.tensor([torch.mean(wealth), torch.var(wealth), 0.0], dtype=torch.float32).to(device)\n",
        "                phi_new = torch.clamp(phi_new, -1e6, 1e6)\n",
        "                phi_path.append(phi_new.unsqueeze(0))\n",
        "        elif method == 'discrete_state':\n",
        "            inputs = torch.cat([\n",
        "                wealth.unsqueeze(-1),\n",
        "                z.repeat(N_grid).unsqueeze(-1),\n",
        "                phi_path[-1].repeat(N_grid, 1)\n",
        "            ], dim=1).requires_grad_(True)  # Enable gradients for inputs\n",
        "            c = policy_fn(inputs)\n",
        "            with torch.no_grad():\n",
        "                dw = env.wealth_drift(wealth, c, z) * dt + env.sigma_w * torch.sqrt(torch.tensor(dt, dtype=torch.float32)) * torch.randn(N_grid).to(device)\n",
        "                wealth = wealth + dw\n",
        "                wealth = torch.clamp(wealth, env.w_min, env.w_max)\n",
        "                phi_new = torch.tensor([torch.mean(wealth), torch.var(wealth), 0.0], dtype=torch.float32).to(device)\n",
        "                phi_new = torch.clamp(phi_new, -1e6, 1e6)\n",
        "                wealth_paths.append(phi_new.cpu().numpy())\n",
        "                phi_path.append(phi_new.unsqueeze(0))\n",
        "        elif method == 'projection':\n",
        "            inputs = torch.cat([\n",
        "                phi_path[-1][:, 0].unsqueeze(-1),\n",
        "                z.unsqueeze(-1),\n",
        "                phi_path[-1]\n",
        "            ], dim=1).requires_grad_(True)  # Enable gradients for inputs\n",
        "            c = policy_fn(inputs)\n",
        "            with torch.no_grad():\n",
        "                dphi = phi_drift_fn(phi_path[-1][:, 0], z, phi_path[-1], None, c, method=method) * dt\n",
        "                dphi = torch.clamp(dphi, -1e6, 1e6)\n",
        "                phi_new = phi_path[-1] + dphi\n",
        "                phi_new = torch.clamp(phi_new, -1e6, 1e6)\n",
        "                wealth_paths.append(phi_new.cpu().numpy())\n",
        "                phi_path.append(phi_new)\n",
        "        # Update aggregate shock\n",
        "        with torch.no_grad():\n",
        "            dz = -env.rho * z * dt + env.sigma_z * torch.sqrt(torch.tensor(dt, dtype=torch.float32)) * torch.randn(1).to(device)\n",
        "            z = z + dz\n",
        "            z_path.append(z.item())\n",
        "\n",
        "    with torch.no_grad():\n",
        "        return np.array(wealth_paths), np.array(z_path), torch.cat(phi_path, dim=0).cpu().numpy()\n",
        ""
      ],
      "metadata": {
        "id": "-IUlvqokHsPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Module 7: Figure and Table Generation\n",
        "-------------------------------------\n",
        "- Visualizes results from EMINN Krusell-Smith reproduction\n",
        "- Plots value function, policy function, wealth distribution, PDE residuals, and relative errors\n",
        "- Generates error tables with PDE residuals and moment errors\n",
        "- Displays plots inline in Colab and saves to output_dir\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "DcaQ2redHuGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ==========================================\n",
        "# 7A. Plot Training Loss\n",
        "# ==========================================\n",
        "def plot_training_loss(history, output_dir=\"./eminn_results/\", method='finite_agent'):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(history, lw=2)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(f\"EMINN Training Loss - {method}\")\n",
        "    plt.grid(True)\n",
        "    plt.savefig(output_dir + f\"training_loss_{method}.png\")\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "VuzQSOolHwJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 7B. Plot Value Function & Policy Function\n",
        "# ==========================================\n",
        "def plot_value_and_policy(V_net, env, method='finite_agent', output_dir=\"./eminn_results/\"):\n",
        "    w_grid = torch.linspace(env.w_min + 1e-4, env.w_max, 200).to(device)\n",
        "    z_low, z_mid, z_high = env.z0 - 0.05, env.z0, env.z0 + 0.05\n",
        "\n",
        "    for z_val, label in zip([z_low, z_mid, z_high], ['Low z', 'Mid z', 'High z']):\n",
        "        z_grid = torch.ones_like(w_grid) * z_val\n",
        "        phi_dummy = torch.zeros(200, 3).to(device)\n",
        "\n",
        "        # Value function\n",
        "        inputs = torch.cat([w_grid.unsqueeze(-1), z_grid.unsqueeze(-1), phi_dummy], dim=1)\n",
        "        V_vals = V_net(inputs).detach().cpu().numpy()\n",
        "\n",
        "        # Policy function\n",
        "        w_req = w_grid.clone().detach().requires_grad_(True)\n",
        "        inputs_policy = torch.cat([w_req.unsqueeze(-1), z_grid.unsqueeze(-1), phi_dummy], dim=1)\n",
        "        V_tmp = V_net(inputs_policy)\n",
        "        V_w = torch.autograd.grad(V_tmp.sum(), w_req, create_graph=False)[0]\n",
        "        c_vals = env.solve_consumption(V_w).detach().cpu().numpy()\n",
        "\n",
        "        # Plot Value Function\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.plot(w_grid.cpu().numpy(), V_vals, label=f\"V(w) {label}\")\n",
        "        plt.xlabel(\"Wealth w\")\n",
        "        plt.ylabel(\"Value Function V\")\n",
        "        plt.title(f\"Value Function - {label} ({method})\")\n",
        "        plt.grid(True)\n",
        "        plt.savefig(output_dir + f\"value_function_{label}_{method}.png\")\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        # Plot Policy Function\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.plot(w_grid.cpu().numpy(), c_vals, label=f\"c(w) {label}\", color='orange')\n",
        "        plt.xlabel(\"Wealth w\")\n",
        "        plt.ylabel(\"Consumption c(w)\")\n",
        "        plt.title(f\"Policy Function - {label} ({method})\")\n",
        "        plt.grid(True)\n",
        "        plt.savefig(output_dir + f\"policy_function_{label}_{method}.png\")\n",
        "        plt.show()\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "yiAHabp8Hx-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 7C. Plot Wealth Distribution Evolution\n",
        "# ==========================================\n",
        "def plot_wealth_distribution(wealth_paths, method='finite_agent', output_dir=\"./eminn_results/\"):\n",
        "    plt.figure(figsize=(7,5))\n",
        "    if method == 'finite_agent':\n",
        "        T_steps = len(wealth_paths)\n",
        "        for idx in [0, int(T_steps/3), int(2*T_steps/3), T_steps-1]:\n",
        "            plt.hist(wealth_paths[idx], bins=30, alpha=0.5, label=f\"t={idx*0.05:.1f}\")\n",
        "        plt.xlabel(\"Wealth w\")\n",
        "        plt.ylabel(\"Density\")\n",
        "        plt.title(f\"Wealth Distribution Evolution ({method})\")\n",
        "        plt.legend()\n",
        "    else:\n",
        "        wealth_paths = np.array(wealth_paths)\n",
        "        times = np.arange(len(wealth_paths)) * 0.05\n",
        "        plt.plot(times, wealth_paths[:, 0], label='Mean wealth')\n",
        "        plt.plot(times, wealth_paths[:, 1], label='Variance', linestyle='--')\n",
        "        plt.plot(times, wealth_paths[:, 2], label='Skewness', linestyle='-.')\n",
        "        plt.xlabel(\"Time t\")\n",
        "        plt.ylabel(\"Moments\")\n",
        "        plt.title(f\"Distribution Moment Evolution ({method})\")\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "    plt.savefig(output_dir + f\"wealth_distribution_evolution_{method}.png\")\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "y-yDgPY3HzqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 7D. Plot PDE Residuals\n",
        "# ==========================================\n",
        "def plot_pde_residuals(V_net, env, method='finite_agent', output_dir=\"./eminn_results/\"):\n",
        "    w_grid = torch.linspace(env.w_min + 1e-4, env.w_max, 100).to(device)\n",
        "    z_grid = torch.linspace(env.z0 - 0.05, env.z0 + 0.05, 100).to(device)\n",
        "    W, Z = torch.meshgrid(w_grid, z_grid, indexing='ij')\n",
        "    phi_dummy = torch.zeros(100 * 100, 3).to(device)\n",
        "\n",
        "    inputs = torch.cat([\n",
        "        W.flatten().unsqueeze(-1),\n",
        "        Z.flatten().unsqueeze(-1),\n",
        "        phi_dummy\n",
        "    ], dim=1)\n",
        "    residual, _, _ = compute_pde_residual(V_net, env, inputs, phi_drift_fn=phi_drift_fn, method=method)\n",
        "    residual = residual.detach().cpu().numpy().reshape(100, 100)\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.contourf(W.cpu().numpy(), Z.cpu().numpy(), np.abs(residual), levels=20)\n",
        "    plt.colorbar(label='|PDE Residual|')\n",
        "    plt.xlabel(\"Wealth w\")\n",
        "    plt.ylabel(\"Aggregate Shock z\")\n",
        "    plt.title(f\"PDE Residuals ({method})\")\n",
        "    plt.savefig(output_dir + f\"pde_residuals_{method}.png\")\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "3WeMu9vzH1JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjDjbj_P2ouP"
      },
      "outputs": [],
      "source": [
        "def generate_error_tables(V_net, env, method='finite_agent', output_dir=\"./eminn_results/\"):\n",
        "    \"\"\"\n",
        "    Generate error metrics and plots:\n",
        "    1. PDE residuals (mean, std, max)\n",
        "    2. Moment errors (mean, variance, skewness)\n",
        "    3. Relative errors for value and policy functions\n",
        "    \"\"\"\n",
        "    # PDE residuals\n",
        "    states = sample_states(env, 5000, method=method)\n",
        "    residual, V_w, V_ww = compute_pde_residual(V_net, env, states, phi_drift_fn=phi_drift_fn, method=method)\n",
        "    res_np = residual.detach().cpu().numpy()\n",
        "    res_np = np.where(np.isnan(res_np) | np.isinf(res_np), 0.0, res_np)  # Replace nan/inf\n",
        "    error_metrics = {\n",
        "        \"Method\": method,\n",
        "        \"Mean Residual\": np.mean(np.abs(res_np)),\n",
        "        \"Std Residual\": np.std(res_np),\n",
        "        \"Max Residual\": np.max(np.abs(res_np))\n",
        "    }\n",
        "\n",
        "    # Moment errors from simulation\n",
        "    policy_fn = get_policy_function(V_net, env)\n",
        "    wealth_paths, z_path, phi_path = simulate_economy(\n",
        "        env, policy_fn, method=method, T=100, dt=0.01,  # Increased T, smaller dt\n",
        "        N_agents=50, N_grid=100, N_basis=3\n",
        "    )\n",
        "    # Analytical steady-state moments (Krusell-Smith)\n",
        "    mean_approx = env.wage * np.exp(env.z0) / env.r  # E[w] = wage*exp(z)/r\n",
        "    var_approx = (env.sigma_w ** 2) / (2 * env.r)   # Var[w] = sigma_w^2/(2r)\n",
        "    skew_approx = 0.0  # Assuming symmetry\n",
        "    phi_final = phi_path[-1]  # Last time step moments (shape: [3])\n",
        "    # Debug: Check phi_path shape\n",
        "    print(\"phi_path shape:\", phi_path.shape)\n",
        "    print(\"phi_final:\", phi_final)\n",
        "    error_metrics.update({\n",
        "        \"Mean Moment Error\": np.abs(phi_final[0] - mean_approx),\n",
        "        \"Variance Moment Error\": np.abs(phi_final[1] - var_approx),\n",
        "        \"Skewness Moment Error\": np.abs(phi_final[2] - skew_approx)\n",
        "    })\n",
        "\n",
        "    # Relative error plots for value and policy functions\n",
        "    w_grid = torch.linspace(env.w_min + 1e-4, env.w_max, 200).to(device)\n",
        "    z_val = torch.tensor(env.z0).to(device)\n",
        "    phi_dummy = torch.zeros(200, 3).to(device)\n",
        "    inputs = torch.cat([w_grid.unsqueeze(-1), z_val.repeat(200).unsqueeze(-1), phi_dummy], dim=1)\n",
        "    V_vals = V_net(inputs).detach().cpu().numpy()\n",
        "    w_req = w_grid.clone().detach().requires_grad_(True)\n",
        "    inputs_policy = torch.cat([w_req.unsqueeze(-1), z_val.repeat(200).unsqueeze(-1), phi_dummy], dim=1)\n",
        "    V_tmp = V_net(inputs_policy)\n",
        "    V_w = torch.autograd.grad(V_tmp.sum(), w_req, create_graph=False)[0]\n",
        "    c_vals = env.solve_consumption(V_w).detach().cpu().numpy()\n",
        "\n",
        "    # Approximate analytical solution\n",
        "    w_np = w_grid.cpu().numpy()\n",
        "    if env.sigma == 2:\n",
        "        V_approx = -(w_np + env.wage * np.exp(env.z0)) ** (-1) / env.r\n",
        "    else:\n",
        "        V_approx = ((w_np + env.wage * np.exp(env.z0)) ** (1 - env.sigma)) / ((1 - env.sigma) * env.r)\n",
        "    c_approx = (env.r * w_np + env.wage * np.exp(env.z0)) * 0.5\n",
        "\n",
        "    rel_error_V = np.abs((V_vals - V_approx) / (np.abs(V_approx) + 1e-8))\n",
        "    rel_error_c = np.abs((c_vals - c_approx) / (np.abs(c_approx) + 1e-8))\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(w_np, rel_error_V, label='Value Function')\n",
        "    plt.plot(w_np, rel_error_c, label='Policy Function')\n",
        "    plt.xlabel(\"Wealth w\")\n",
        "    plt.ylabel(\"Relative Error\")\n",
        "    plt.title(f\"Relative Error - {method}\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(output_dir + f\"relative_error_{method}.png\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    df = pd.DataFrame([error_metrics])\n",
        "    df.to_csv(output_dir + f\"error_table_{method}.csv\", index=False)\n",
        "    print(f\"Error table saved at {output_dir}error_table_{method}.csv\")\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    params = {\n",
        "        'beta': 0.98, 'r': 0.03, 'wage': 1.0, 'rho': 0.95, 'sigma_z': 0.02,\n",
        "        'z0': 0.0, 'w_min': 0.0, 'w_max': 5.0, 'sigma': 2.0, 'sigma_w': 0.01,\n",
        "        'lambda_pen': 10.0\n",
        "    }\n",
        "    env = EconomicEnvironment(params)\n",
        "    methods = ['finite_agent', 'discrete_state', 'projection']\n",
        "    error_dfs = []\n",
        "    output_dir = \"./eminn_results/\"\n",
        "    for method in methods:\n",
        "        V_net = ValueNet(input_dim=1 + 1 + 3).to(device)\n",
        "        print(f\"Starting training using {method} method...\")\n",
        "        V_net, history = train_eminn(V_net, env, epochs=1000, batch_size=2048, lr=1e-3, method=method, verbose=True)\n",
        "        plot_training_loss(history, output_dir=output_dir, method=method)\n",
        "        policy_fn = get_policy_function(V_net, env)\n",
        "        wealth_paths, z_path, phi_path = simulate_economy(env, policy_fn, method=method, T=100, dt=0.01)\n",
        "        plot_value_and_policy(V_net, env, method=method, output_dir=output_dir)\n",
        "        plot_wealth_distribution(wealth_paths, method=method, output_dir=output_dir)\n",
        "        plot_pde_residuals(V_net, env, method=method, output_dir=output_dir)\n",
        "        df_errors = generate_error_tables(V_net, env, method=method, output_dir=output_dir)\n",
        "        error_dfs.append(df_errors)\n",
        "        print(f\"Results for {method}:\\n\", df_errors)\n",
        "    combined_df = pd.concat(error_dfs, ignore_index=True)\n",
        "    combined_df.to_csv(output_dir + \"combined_error_table.csv\", index=False)\n",
        "    print(f\"\\nâœ… Full EMINN pipeline completed for all methods!\")\n",
        "    print(\"Combined error table saved at:\", output_dir + \"combined_error_table.csv\")\n",
        "    print(combined_df)"
      ],
      "metadata": {
        "id": "0rY7LGYuIASS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}